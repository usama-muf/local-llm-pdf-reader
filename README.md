# ğŸ“„ Local LLM PDF Reader with FAISS + Gradio

A privacy-first, fully local AI application that lets you ask questions about your **confidential PDF documents** using **local language models**, **FAISS vector search**, and a **Gradio chatbot UI**.

---

## ğŸš€ Project Overview

This project demonstrates how to build a Retrieval-Augmented Generation (RAG)-style PDF assistant using:

- ğŸ§  A **local LLM** (via `llama-cpp`)
- ğŸ” **FAISS** for fast semantic similarity search
- ğŸ’¡ **SentenceTransformer** for text embeddings
- ğŸ“„ **PyMuPDF (`fitz`)** for extracting text from PDFs
- ğŸ§° A clean **Gradio** interface for interaction

This solution runs **100% locally** (no OpenAI or external API), making it ideal for private or air-gapped environments.

---

## ğŸ§© Features

- ğŸ“¥ Upload any PDF file
- ğŸ§  Automatically chunk and vectorize its content
- ğŸ” Ask natural language questions
- ğŸ¤– Receive LLM-generated answers based only on the PDF content
- ğŸ” Works fully offline â€“ privacy preserved
- ğŸ–¥ï¸ Designed for local development and personal use

---

## ğŸ“¦ Tech Stack

| Component              | Technology           |
|------------------------|----------------------|
| Local LLM Inference    | `llama-cpp-python`   |
| Embeddings             | `sentence-transformers` (MiniLM) |
| Vector DB              | `FAISS`              |
| PDF Parsing            | `PyMuPDF` (`fitz`)   |
| Frontend UI            | `Gradio`             |
| Environment            | Python 3.8+ & WSL/Ubuntu |

---

## ğŸ–¼ï¸ UI Preview

### ğŸ”¹ Screenshot 1
![Screenshot 1](screenshots/ss1.png)

### ğŸ”¹ Screenshot 2
![Screenshot 2](screenshots/ss2.png)

---

## ğŸ› ï¸ How It Works

1. **PDF Parsing**  
   The PDF is opened using `PyMuPDF`, and its text is extracted page-wise. If the PDF is image-based, youâ€™ll need OCR support (future enhancement).

2. **Chunking**  
   The raw text is split into manageable chunks (e.g., 500 characters).

3. **Embedding & Indexing**  
   Each chunk is embedded using a pre-trained SentenceTransformer and indexed in FAISS.

4. **Semantic Search + RAG**  
   When a user asks a question:
   - We embed the query
   - Retrieve top similar chunks using FAISS
   - Construct a prompt containing relevant context
   - Feed to the local LLM for answer generation

---

## ğŸ“‚ Folder Structure

pdfReaderLLM/
â”œâ”€â”€ models/ # Place your .gguf LLM file here
â”œâ”€â”€ screenshots/ # UI demo images for README
â”œâ”€â”€ app.py # Main Gradio + LLM code
â”œâ”€â”€ venv/ # Python virtual environment (gitignored)
â”œâ”€â”€ requirements.txt # All Python dependencies
â””â”€â”€ README.md # This file


---

## â–¶ï¸ Running the App

### 1. Create Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```
Make sure you have your .gguf model downloaded in the models/ folder.

### 3. Run the app
```bash
python app.py
```

---
## ğŸ“Œ Limitations

- Doesnâ€™t support image-only PDFs *(OCR support planned)*
- Requires a compatible `.gguf` model (e.g., LLaMA, Mistral, etc.)
- Not production-optimized; built for learning and personal research

---
## ğŸ“š Sample Use Cases

- Reading & querying long technical PDFs  
- Summarizing legal documents  
- Creating privacy-safe AI assistants for documents

---

## ğŸ§‘â€ğŸ’» Author

**Usama Mohammed**  
Built for privacy-first AI exploration with local LLMs.

---

## ğŸ›¡ï¸ License

**MIT License** â€“ use freely, modify safely.

---

## â­ï¸ Star the Repo

If this helped or inspired you, consider giving it a â­ï¸ on GitHub!


